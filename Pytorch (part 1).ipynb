{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Tensor Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "import torch                                    \n",
    "#Tensors are simply mathematical objects that can be used to describe physical properties, just like scalars and vectors. In \n",
    "#fact tensors are merely a generalisation of scalars and vectors; a scalar is a zero rank tensor, and a vector is a first rank \n",
    "#tensor.\n",
    "x = torch.empty(1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.4013e-45,  0.0000e+00, -8.8451e+18,  4.5912e-41])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(4)     # 1D vector with 4 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]]])\n",
      "tensor([[[[1.0102e-38, 1.0286e-38],\n",
      "          [1.0194e-38, 9.6429e-39]],\n",
      "\n",
      "         [[9.2755e-39, 9.1837e-39],\n",
      "          [9.3674e-39, 1.0745e-38]]],\n",
      "\n",
      "\n",
      "        [[[1.0653e-38, 9.5510e-39],\n",
      "          [1.0561e-38, 1.0194e-38]],\n",
      "\n",
      "         [[1.1112e-38, 1.0561e-38],\n",
      "          [9.9184e-39, 1.0653e-38]]]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.empty(2,3))         # 2D \n",
    "print(torch.empty(2,2,4))       # 3D\n",
    "print(torch.empty(2,2,2,2))     # 4D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4836, 0.4567, 0.4983],\n",
       "        [0.8350, 0.3400, 0.7881]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(4,4)                     # can use like numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,2, dtype = torch.float16)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,2, dtype = torch.double)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1],\n",
       "        [1, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.ones(2,2, dtype = torch.int)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.6000, 5.0000, 6.0000])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can also create a tensor\n",
    "torch.tensor([3.6, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9504, 0.6960],\n",
      "        [0.9970, 0.3403]])\n",
      "tensor([[0.4174, 0.1551],\n",
      "        [0.2147, 0.8817]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(2,2)\n",
    "y = torch.rand(2,2)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3678, 0.8511],\n",
       "        [1.2117, 1.2220]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = x + y \n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3678, 0.8511],\n",
       "        [1.2117, 1.2220]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9663, 0.6606],\n",
       "        [1.0507, 0.7550]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2,2)\n",
    "y = torch.rand(2,2)\n",
    "y.add_(x)                            #every function having an underscore will perform an inplace operation in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2955, 0.2196],\n",
       "        [0.2037, 0.1249]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#similarly\n",
    "torch.sub(x,y)\n",
    "y.sub_(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0892, 0.0188],\n",
      "        [0.1238, 0.0312]])\n",
      "tensor([[0.0892, 0.0188],\n",
      "        [0.1238, 0.0312]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.mul(x,y))\n",
    "print(y.mul_(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.5210, 23.4152],\n",
       "        [ 6.8424, 20.1642]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.div(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4045, 0.9259, 0.0204, 0.1073],\n",
      "        [0.2326, 0.4080, 0.6381, 0.0708],\n",
      "        [0.8217, 0.5334, 0.9933, 0.0996],\n",
      "        [0.3481, 0.3111, 0.7953, 0.3522],\n",
      "        [0.1089, 0.1623, 0.3133, 0.7899]])\n",
      "tensor([0.4045, 0.2326, 0.8217, 0.3481, 0.1089])\n",
      "tensor([0.2326, 0.4080, 0.6381, 0.0708])\n",
      "tensor(0.4080)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5,4)                  #slicing\n",
    "print(x)\n",
    "print(x[:,0])\n",
    "print(x[1,:])\n",
    "print(x[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9153, 0.4827, 0.2136, 0.8976],\n",
      "        [0.6122, 0.7277, 0.4785, 0.1733],\n",
      "        [0.8557, 0.8053, 0.1484, 0.5252],\n",
      "        [0.7970, 0.8413, 0.4391, 0.9449]])\n",
      "tensor([0.9153, 0.4827, 0.2136, 0.8976, 0.6122, 0.7277, 0.4785, 0.1733, 0.8557,\n",
      "        0.8053, 0.1484, 0.5252, 0.7970, 0.8413, 0.4391, 0.9449])\n"
     ]
    }
   ],
   "source": [
    "#reshaping\n",
    "x = torch.rand(4,4)\n",
    "print(x)\n",
    "print(x.view(16))        # 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4443, 0.6559, 0.4730, 0.7609],\n",
      "        [0.9273, 0.6397, 0.3965, 0.9528],\n",
      "        [0.9079, 0.2264, 0.5950, 0.7351],\n",
      "        [0.4473, 0.9228, 0.6103, 0.2132]])\n",
      "tensor([[0.4443, 0.6559, 0.4730, 0.7609, 0.9273, 0.6397, 0.3965, 0.9528],\n",
      "        [0.9079, 0.2264, 0.5950, 0.7351, 0.4473, 0.9228, 0.6103, 0.2132]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can resize the tensors\n",
    "x = torch.rand(4,4)\n",
    "print(x)\n",
    "print(x.view(-1,8))\n",
    "x.view(-1,8).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# onverting from numpy to tensor or vice versa\n",
    "import torch\n",
    "import numpy as np\n",
    "a = torch.ones(5)\n",
    "print(a)\n",
    "b = a.numpy()\n",
    "print(b)\n",
    "print(type(b))   \n",
    "# note down that if the tensors are on the cpu and not the gpu then both the tensors will share the same memory location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1.]\n",
      "tensor([1., 1., 1., 1.], dtype=torch.float64)\n",
      "[2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a = np.ones(4)\n",
    "print(a)\n",
    "b = torch.from_numpy(a)\n",
    "print(b)\n",
    "a += 1\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    x = torch.ones(5,device = device)\n",
    "    y = torch.ones(5)\n",
    "    y = y.to(device)\n",
    "    z = x + y\n",
    "    z = z.to('cpu')\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(5, requires_grad = True)   #so this will tell pytorch to calculate the gradients later for this tensor for \n",
    "                                        #optimization steps, by default it is false\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7484,  0.0254, -1.1034], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#Autograd package is used to calculate the gradients which is essential for model optimization\n",
    "import torch\n",
    "\n",
    "x = torch.randn(3, requires_grad = True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2516, 2.0254, 0.8966], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#to calculate gradients there will be forward pass to calculate y here and since requires_grad is specified as True pytorch\n",
    "#automatically provides a function to calculate the gradients using the back propagation method. It contains an attribute \n",
    "#namely grad_fn and performs the operation to calculate the gradients of y wrt x (dy/dx), since it as an addition operation \n",
    "#it is shown as AddBackward in the output\n",
    "y = x+2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.1329, 8.2042, 1.6078], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = y*y*2\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3912, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = y.mean()\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.3333, 1.3333, 1.3333])\n"
     ]
    }
   ],
   "source": [
    "#the only thing that must be done to calculate gradient is \n",
    "z.backward()  #dz/dx\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5682,  1.6061, -0.5940])\n",
      "tensor([2.5682, 3.6061, 1.4060])\n",
      "tensor(14.3841)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-e82a03e094d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "import torch                  #if don't specify the argument\n",
    "\n",
    "x = torch.randn(3, requires_grad = False)\n",
    "print(x)\n",
    "y = x+2\n",
    "print(y)\n",
    "z = y*y*2\n",
    "z = z.mean()\n",
    "print(z)\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.3998, -2.0988,  0.5052], requires_grad=True)\n",
      "tensor([ 0.6002, -0.0988,  2.5052], grad_fn=<AddBackward0>)\n",
      "tensor([ 0.7204,  0.0195, 12.5521], grad_fn=<MulBackward0>)\n",
      "tensor([ 0.2401, -0.3952, 20.0417])\n"
     ]
    }
   ],
   "source": [
    "import torch                #vector jacobian product\n",
    "\n",
    "x = torch.randn(3, requires_grad = True)\n",
    "print(x)\n",
    "y = x+2\n",
    "print(y)\n",
    "z = y*y*2\n",
    "#z = z.mean()\n",
    "v = torch.tensor([0.1,1.0,2.0], dtype = torch.float32)\n",
    "print(z)\n",
    "z.backward(v)\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1927, 0.4535, 0.3259], requires_grad=True)\n",
      "tensor([0.1927, 0.4535, 0.3259])\n",
      "tensor([0.1927, 0.4535, 0.3259])\n",
      "tensor([2.1927, 2.4535, 2.3259])\n",
      "tensor([2.1927, 2.4535, 2.3259])\n"
     ]
    }
   ],
   "source": [
    "import torch             \n",
    "\n",
    "x = torch.randn(3, requires_grad = True)\n",
    "print(x)\n",
    "#we should also know to prevent tracking gradients if we don't need it \n",
    "# x.requires_grad_(false)\n",
    "# x.detach()                 #it will create a new tensor that doesn't require a gradient\n",
    "# with torch.no_grad():\n",
    "x.requires_grad_(False)\n",
    "print(x)\n",
    "\n",
    "y = x.detach()\n",
    "print(y)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = x + 2\n",
    "    print(y)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4., 4., 4., 4.])\n",
      "tensor([4., 4., 4., 4.])\n",
      "tensor([4., 4., 4., 4.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "weights = torch.ones(4, requires_grad = True)\n",
    "for epoch in range(3):\n",
    "    model_output = (weights*4).sum()\n",
    "    model_output.backward()\n",
    "    print(weights.grad)\n",
    "    \n",
    "    weights.grad.zero_()                        #to get back the gradients to initial values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have a pytorvh buily in optimizer\n",
    "import torch\n",
    "weights = torch.ones(3, requires_grad = True)\n",
    "\n",
    "optimizer = torch.optim.SGD(weights, lr=0.01)      #SGD = Stochastic Gradient Descent, lr = learning rate\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'z' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-da1f015e5aa5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'z' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "weights = torch.ones(4, requires_grad = True)\n",
    "\n",
    "z.backward()\n",
    "weights.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n",
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "#we must know the chain rule for backpropagation and we calculate the local gradients and lastly calculate the loss function\n",
    "# which we want to minimize\n",
    "#This whole process consists of 3 steps in short\n",
    "# 1) forward pass: compute loss\n",
    "# 2) compute local gradients\n",
    "# 3) backward pass: compute dLoss/dWeights using the chain rule  (we compute the gradient of the loss) \n",
    "#example x=1, y=2, w=1\n",
    "import torch\n",
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "w = torch.tensor(1.0, requires_grad = True)\n",
    "\n",
    "#forwad pass and compute the loss\n",
    "y_cap = w*x\n",
    "loss = (y_cap - y)**2\n",
    "\n",
    "print(loss)\n",
    "\n",
    "#backwardpass  (pytorch automatically computes the local gradients for us and also the backward pass)\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "\n",
    "### update weights\n",
    "### next forward and backwardpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent using Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch1: w = 1.200, loss = 30.00000000\n",
      "epoch3: w = 1.872, loss = 0.76800019\n",
      "epoch5: w = 1.980, loss = 0.01966083\n",
      "epoch7: w = 1.997, loss = 0.00050332\n",
      "epoch9: w = 1.999, loss = 0.00001288\n",
      "epoch11: w = 2.000, loss = 0.00000033\n",
      "epoch13: w = 2.000, loss = 0.00000001\n",
      "epoch15: w = 2.000, loss = 0.00000000\n",
      "epoch17: w = 2.000, loss = 0.00000000\n",
      "epoch19: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# we can optimize our model by calculating automatic gradient computation using the autograd package    \n",
    "# 1) Prediction : pytorch model\n",
    "# 2) Gradients computation : autograd\n",
    "# 3) Loss Computation : pytorch loss\n",
    "# 4) Parameter updates : pytorch optimizer\n",
    "\n",
    "#step 1\n",
    "import numpy as np\n",
    "\n",
    "#f = w * x   \n",
    "#f = 2 * x\n",
    "X = np.array([1,2,3,4], dtype = np.float32)\n",
    "Y = np.array([2,4,6,8],dtype = np.float32)\n",
    "\n",
    "w = 0.0    #initialising weights\n",
    "\n",
    "#Model prediction\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "#loss = MSE\n",
    "def loss(y,y_predicted):\n",
    "    return ((y_predicted - y)**2).mean()\n",
    "    \n",
    "#gradient\n",
    "#MSE = 1/N * (w*x-y)**2\n",
    "# dJ/dw = 1/N 2x (w*x-y)\n",
    "def gradient(x,y,y_predicted):\n",
    "    return np.dot(2*x, y_predicted-y).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "#training\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    #loss\n",
    "    l = loss(Y,y_pred)\n",
    "    \n",
    "    #gradients\n",
    "    dw = gradient(X,Y,y_pred)\n",
    "    \n",
    "    #update weights\n",
    "    w -= learning_rate*dw\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        print(f'epoch{epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')        \n",
    "\n",
    "\n",
    "#here we calculate every step manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch1: w = 0.300, loss = 30.00000000\n",
      "epoch3: w = 0.772, loss = 15.66018772\n",
      "epoch5: w = 1.113, loss = 8.17471695\n",
      "epoch7: w = 1.359, loss = 4.26725292\n",
      "epoch9: w = 1.537, loss = 2.22753215\n",
      "epoch11: w = 1.665, loss = 1.16278565\n",
      "epoch13: w = 1.758, loss = 0.60698116\n",
      "epoch15: w = 1.825, loss = 0.31684780\n",
      "epoch17: w = 1.874, loss = 0.16539653\n",
      "epoch19: w = 1.909, loss = 0.08633806\n",
      "Prediction after training: f(5) = 9.612\n"
     ]
    }
   ],
   "source": [
    "# we can optimize our model by calculating automatic gradient computation using the autograd package\n",
    "# 1) Prediction : pytorch model\n",
    "# 2) Gradients computation : autograd\n",
    "# 3) Loss Computation : pytorch loss\n",
    "# 4) Parameter updates : pytorch optimizer\n",
    "\n",
    "#step 2\n",
    "import torch\n",
    "\n",
    "#f = w * x   \n",
    "#f = 2 * x\n",
    "X = torch.tensor([1,2,3,4], dtype = torch.float32)\n",
    "Y = torch.tensor([2,4,6,8], dtype = torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype = torch.float32, requires_grad = True)    #initialising weights\n",
    "\n",
    "#Model prediction\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "#loss = MSE\n",
    "def loss(y,y_predicted):\n",
    "    return ((y_predicted - y)**2).mean()\n",
    "    \n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "#training\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    #loss\n",
    "    l = loss(Y,y_pred)\n",
    "    \n",
    "    #gradients = backward pass\n",
    "    l.backward()  #dl/dw\n",
    "    \n",
    "    #update weights\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "    \n",
    "    #zero gradients\n",
    "    w.grad.zero_()\n",
    "    \n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        print(f'epoch{epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch1: w = 0.300, loss = 30.00000000\n",
      "epoch11: w = 1.665, loss = 1.16278565\n",
      "epoch21: w = 1.934, loss = 0.04506890\n",
      "epoch31: w = 1.987, loss = 0.00174685\n",
      "epoch41: w = 1.997, loss = 0.00006770\n",
      "epoch51: w = 1.999, loss = 0.00000262\n",
      "epoch61: w = 2.000, loss = 0.00000010\n",
      "epoch71: w = 2.000, loss = 0.00000000\n",
      "epoch81: w = 2.000, loss = 0.00000000\n",
      "epoch91: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# we can optimize our model by calculating automatic gradient computation using the autograd package\n",
    "# 1) Prediction : pytorch model\n",
    "# 2) Gradients computation : autograd\n",
    "# 3) Loss Computation : pytorch loss\n",
    "# 4) Parameter updates : pytorch optimizer\n",
    "\n",
    "#step 2\n",
    "import torch\n",
    "\n",
    "#f = w * x   \n",
    "#f = 2 * x\n",
    "X = torch.tensor([1,2,3,4], dtype = torch.float32)\n",
    "Y = torch.tensor([2,4,6,8], dtype = torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype = torch.float32, requires_grad = True)    #initialising weights\n",
    "\n",
    "#Model prediction\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "#loss = MSE\n",
    "def loss(y,y_predicted):\n",
    "    return ((y_predicted - y)**2).mean()\n",
    "    \n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "#training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    #loss\n",
    "    l = loss(Y,y_pred)\n",
    "    \n",
    "    #gradients = backward pass\n",
    "    l.backward()  #dl/dw\n",
    "    \n",
    "    #update weights\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "    \n",
    "    #zero gradients\n",
    "    w.grad.zero_()\n",
    "    \n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'epoch{epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training pipeline\n",
    "Model / Loss / optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch1: w = 0.300, loss = 30.00000000\n",
      "epoch11: w = 1.665, loss = 1.16278565\n",
      "epoch21: w = 1.934, loss = 0.04506890\n",
      "epoch31: w = 1.987, loss = 0.00174685\n",
      "epoch41: w = 1.997, loss = 0.00006770\n",
      "epoch51: w = 1.999, loss = 0.00000262\n",
      "epoch61: w = 2.000, loss = 0.00000010\n",
      "epoch71: w = 2.000, loss = 0.00000000\n",
      "epoch81: w = 2.000, loss = 0.00000000\n",
      "epoch91: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "#The general training pipeline in pytorch\n",
    "# 1) Design Model (input, output size, forward pass)\n",
    "# 2) Construct loss and optimizer\n",
    "# 3) Training loop\n",
    "# - forward pass: compute prediction\n",
    "# - backward pass: gradients\n",
    "# - update weights\n",
    "\n",
    "#step 3\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "#f = w * x   \n",
    "#f = 2 * x\n",
    "X = torch.tensor([1,2,3,4], dtype = torch.float32)\n",
    "Y = torch.tensor([2,4,6,8], dtype = torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype = torch.float32, requires_grad = True)    #initialising weights\n",
    "\n",
    "#Model prediction\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "    \n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "#training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w], lr = learning_rate)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    #loss\n",
    "    l = loss(Y,y_pred)\n",
    "    \n",
    "    #gradients = backward pass\n",
    "    l.backward()  #dl/dw\n",
    "    \n",
    "    #update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    #zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'epoch{epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "Prediction before training: f(5) = 2.167\n",
      "epoch1: w = 0.798, loss = 20.34970093\n",
      "epoch11: w = 1.830, loss = 0.52759916\n",
      "epoch21: w = 1.995, loss = 0.01468987\n",
      "epoch31: w = 2.021, loss = 0.00135930\n",
      "epoch41: w = 2.025, loss = 0.00095737\n",
      "epoch51: w = 2.025, loss = 0.00089330\n",
      "epoch61: w = 2.024, loss = 0.00084110\n",
      "epoch71: w = 2.023, loss = 0.00079213\n",
      "epoch81: w = 2.023, loss = 0.00074603\n",
      "epoch91: w = 2.022, loss = 0.00070260\n",
      "Prediction after training: f(5) = 10.044\n"
     ]
    }
   ],
   "source": [
    "#step 4\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "#f = w * x   \n",
    "#f = 2 * x\n",
    "X = torch.tensor([[1],[2],[3],[4]], dtype = torch.float32)          #we need to keep the inputs in 2D when we are creating \n",
    "                                                                    #the model using torch and here the rows represent number\n",
    "                                                                    #of samples    \n",
    "Y = torch.tensor([[2],[4],[6],[8]], dtype = torch.float32) \n",
    "X_test = torch.tensor([5], dtype = torch.float32)\n",
    "\n",
    "n_samples,n_features = X.shape\n",
    "print(n_samples,n_features)\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "    \n",
    "\n",
    "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "#training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    #loss\n",
    "    l = loss(Y,y_pred)\n",
    "    \n",
    "    #gradients = backward pass\n",
    "    l.backward()  #dl/dw\n",
    "    \n",
    "    #update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    #zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        [w, b] = model.parameters()\n",
    "        print(f'epoch{epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "Prediction before training: f(5) = 10.044\n",
      "epoch1: w = 2.021, loss = 0.00066171\n",
      "epoch11: w = 2.021, loss = 0.00062320\n",
      "epoch21: w = 2.020, loss = 0.00058692\n",
      "epoch31: w = 2.020, loss = 0.00055276\n",
      "epoch41: w = 2.019, loss = 0.00052058\n",
      "epoch51: w = 2.018, loss = 0.00049028\n",
      "epoch61: w = 2.018, loss = 0.00046174\n",
      "epoch71: w = 2.017, loss = 0.00043487\n",
      "epoch81: w = 2.017, loss = 0.00040955\n",
      "epoch91: w = 2.016, loss = 0.00038572\n",
      "Prediction after training: f(5) = 10.033\n"
     ]
    }
   ],
   "source": [
    "#step 4\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "#f = w * x   \n",
    "#f = 2 * x\n",
    "X = torch.tensor([[1],[2],[3],[4]], dtype = torch.float32)          #we need to keep the inputs in 2D when we are creating \n",
    "                                                                    #the model using torch and here the rows represent number\n",
    "                                                                    #of samples    \n",
    "Y = torch.tensor([[2],[4],[6],[8]], dtype = torch.float32) \n",
    "X_test = torch.tensor([5], dtype = torch.float32)\n",
    "\n",
    "n_samples,n_features = X.shape\n",
    "print(n_samples,n_features)\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "#model = nn.Linear(input_size, output_size)\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        #define layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "#training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    #loss\n",
    "    l = loss(Y,y_pred)\n",
    "    \n",
    "    #gradients = backward pass\n",
    "    l.backward()  #dl/dw\n",
    "    \n",
    "    #update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    #zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        [w, b] = model.parameters()\n",
    "        print(f'epoch{epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {model(X_test).item():.3f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
